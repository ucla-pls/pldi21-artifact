{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the notebook, which contains the results of running our evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy.stats as stats\n",
    "from pathlib import Path\n",
    "\n",
    "def pretty(ax):\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    \n",
    "    for spine in [ax.spines['left'], ax.spines['bottom']]:\n",
    "        spine.set_position((\"outward\", 5))\n",
    "        spine.set_color(\"gray\")\n",
    "        \n",
    "    for axis in [ax.yaxis, ax.xaxis]:\n",
    "        for x in axis.get_major_ticks():\n",
    "            x.label1.set_color(\"gray\")\n",
    "            x.tick1line.set_color(\"gray\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full\n",
    "\n",
    "This section covers the evaluation where we preserve the full bug. We start by loading the the data and indexing by `name`, `predicate`, and `strategy`. The data have been computed and put in `results/result.csv` by our evalutation framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(\"result/full/result.csv\").set_index([\"name\", \"predicate\",\"strategy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A single line of our data looks like this, we store the follwing data: \n",
    "\n",
    "*  `bugs` which contain the number of lines in the cleaned up bug-report\n",
    "\n",
    "*  `initial-scc` and `scc` contain the number of strongly connected components before and after reduction,\n",
    "\n",
    "*  `initial-classes` and `classes` contain the number of classes before and after reduction,\n",
    "\n",
    "*  `initial-bytes` and `bytes` contain the number of bytes before and after reduction,\n",
    "\n",
    "*  `iters` which contain the number of invocations of the predicate, \n",
    "\n",
    "*  `searches` the number of binary searches made by algorithm\n",
    "\n",
    "*  `time` which records the time to reach the final successfull solution,\n",
    "\n",
    "*  `status` which records whether the reduction completed correctly,\n",
    "\n",
    "*  `verify` which records information about if bug is preserved.\n",
    "\n",
    "Here is an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[\"url0067cdd33d_goldolphin_Mi\", \"cfr\", \"classes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty(results.bugs.plot.hist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are testing four startegies: \n",
    "\n",
    "- classes\n",
    "- logic+approx\n",
    "- logic+graph\n",
    "- logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = list(reversed([\"classes\", \"logic+ddmin\", \"logic+ddmin+rev\", \"logic+approx\", \"logic+approx+rev\", \"logic\"]))\n",
    "colors = {\"initial\": \"gray\", \n",
    "          \"classes\" : \"red\", \n",
    "          \"logic+approx\" : \"orange\", \"logic+approx+rev\" : \"gold\", \n",
    "          \"logic+graph\" : \"orange\", \n",
    "          \"logic+ddmin\" : \"green\", \"logic+ddmin+rev\" : \"lightgreen\", \n",
    "          \"logic\": \"blue\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Sanity Checks\n",
    "\n",
    "Before we go on to evaluate the code we check that the system is working correctly. First we check that the status is \"success\". We find the following distribution of statuses:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(14,2))\n",
    "\n",
    "timeouts = (results.status == \"timeout\").groupby(\"strategy\").mean()\n",
    "\n",
    "ax.set_xlim(0, 100)\n",
    "\n",
    "pretty(ax)\n",
    "x = ax.barh(\n",
    "        strategies, \n",
    "        [timeouts[s] * 100 for s in strategies], \n",
    "        color=[colors[s] for s in strategies]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a list of all the experiments that failed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10000000\n",
    "for i in results.index:\n",
    "    (b, p, x) = i\n",
    "    if x != \"classes\": continue\n",
    "    if results.classes.loc[(b, p, x)] < results.classes.loc[(b, p, \"logic\")]:\n",
    "        if results[\"initial-bytes\"][i] < m:\n",
    "            m = results[\"initial-bytes\"][i]\n",
    "            print('/'.join(i), results[\"initial-bytes\"][i], results.bytes[i], results.bytes[(b, p, \"logic\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our comparative reducetion results we will update the times of all timeout items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMEOUT = 3600\n",
    "full = results.copy()\n",
    "full.time.loc[full.status == \"timeout\"] = TIMEOUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our first experiment we are going to look at comparative final size, and time. We use the geometric mean, so that we can compare the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyvalues = full.filter([\"bytes\", \"classes\", \"time\"], axis=1)\\\n",
    "    .unstack(\"strategy\")\\\n",
    "    .agg(stats.gmean)\\\n",
    "    .unstack()\n",
    "\n",
    "v = full.filter([\"initial-bytes\", \"initial-classes\"], axis=1).unstack(\"strategy\")\\\n",
    "    .agg(stats.gmean)\\\n",
    "    .unstack()[\"classes\"]\\\n",
    "    .rename(lambda a: a.lstrip(\"initial-\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyvalues.round(1)[list(reversed(strategies))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(keyvalues.loc[[\"bytes\",\"classes\"]].div(v, axis='rows') * 100).round(1)[list(reversed(strategies))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphical Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_diagram(full):\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(12,4), sharey=True)\n",
    "\n",
    "    for lb, ax in zip([\"time\", \"iters\", \"classes\", \"bytes\"], axes.flatten()):\n",
    "        maxx, minx = 0, 1000000000\n",
    "       \n",
    "        x = full[lb].unstack(\"strategy\")\n",
    "        if lb != \"time\" and lb != \"iters\":\n",
    "            x = x / full[\"initial-\" + lb].unstack(\"strategy\")\n",
    "        for key in strategies:\n",
    "            if lb == \"time\":\n",
    "                data = list(sorted(d for d in x[key] if d < TIMEOUT))\n",
    "                data.append(TIMEOUT)\n",
    "            else:\n",
    "                data = sorted(x[key])\n",
    "            \n",
    "            ax.plot(data, [i + 1 for i,_ in enumerate(data)], label=key, color=colors[key])\n",
    "            maxx = max(maxx, max(x[key]))\n",
    "            minx = min(minx, min(x[key]))\n",
    "            \n",
    "           \n",
    "        # if lb == \"bytes\" or lb == \"classes\":\n",
    "        #     x = full[\"initial-\" + lb].unstack(\"strategy\")\n",
    "        #     ax.plot(sorted(x[\"classes\"]), \n",
    "        #             [i + 1 for i,_ in enumerate(x[\"classes\"])],\n",
    "        #             label=\"initial\", color=colors[\"initial\"])\n",
    "        #     maxx = max(maxx, max(x[\"classes\"]))\n",
    "        #     minx = min(minx, min(x[\"classes\"]))\n",
    "        # else:\n",
    "        #     ax.plot([], [], label=\"initial\", color=colors[\"initial\"])\n",
    "            \n",
    "        minx = max(1, minx)\n",
    "        \n",
    "        ylim = 0, len(x[key])\n",
    "        ax.set_yticks(np.linspace(*ylim, 7))\n",
    "        ax.set_ylim(*ylim)\n",
    "        \n",
    "        if lb == \"time\":\n",
    "            xlim = 0, 3600\n",
    "            ax.set_xticks(np.linspace(*xlim, 5))\n",
    "        elif lb == \"iters\":\n",
    "            xlim = 0, maxx\n",
    "            l = np.linspace(*xlim, 5)\n",
    "            ax.set_xticks(l)\n",
    "        else:\n",
    "            xlim = 0,1\n",
    "            #ax.set_xscale(\"log\")\n",
    "            # print(lb, xlim)\n",
    "            l = np.logspace(math.log(minx,2), math.log(maxx,2), 5, base=2)\n",
    "            l = np.linspace(*xlim, 5)\n",
    "            ax.set_xticks(l)\n",
    "        ax.set_xlim(*xlim)\n",
    "        \n",
    "        #if lb == \"bytes\":\n",
    "        #    ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, pos: f'{x/1000:0.0f} KB'))\n",
    "        if lb == \"time\":\n",
    "            ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, pos: f'{x:0.0f} s'))\n",
    "        elif lb == \"iters\":\n",
    "            ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, pos: f'{x:0.0f}'))\n",
    "        else:\n",
    "            ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, pos: f'{100 * x:0.0f}%'))\n",
    "        v = ax.hlines(102,*xlim)\n",
    "        v.set_color(\"lightgray\")\n",
    "        v.set_linestyle(\":\")\n",
    "        \n",
    "        ax.set_xlabel(lb)    \n",
    "        pretty(ax)\n",
    "    \n",
    "    axes[3].legend()\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "fig = draw_diagram(full)\n",
    "fig.savefig(\"new-graph.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphs are formatted like the previous article: In the top row we have number programs that complete before a certain time and iterations. In the bottom row we have the number of programs that have been reduced to a size below a certian number of bytes or classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "glob = Path(\"result/full/\").glob(\"url*/*\")\n",
    "dfCs = []\n",
    "dfBs = []\n",
    "j = 0 \n",
    "for b in glob:\n",
    "    if not b.name in {\"cfr\", \"procyon\", \"fernflower\"}: continue\n",
    "    j += 1\n",
    "    #if j > 10: break\n",
    "    try:\n",
    "        dfC = pd.DataFrame()\n",
    "        dfB = pd.DataFrame()\n",
    "        metrics = list(b.glob(\"*/workfolder/metrics.csv\"))\n",
    "        if not metrics: continue\n",
    "        for i in metrics:\n",
    "            strat, *_ = i.relative_to(b).parts\n",
    "            # print(i)\n",
    "            m = pd.read_csv(i)\n",
    "            m.time = m.time.floordiv(60) + 1\n",
    "            x = m[m.judgment == \"success\"]\\\n",
    "                .groupby(\"time\")[[\"classes\", \"bytes\"]]\\\n",
    "                .min()\\\n",
    "                .div(m.iloc[0][[\"classes\", \"bytes\"]])\\\n",
    "                .reindex(index=range(0, 61))\\\n",
    "                .expanding().min()\\\n",
    "                .fillna(1)\n",
    "        \n",
    "            dfC = dfC.assign(**{strat: x[\"classes\"]})\n",
    "            dfB = dfB.assign(**{strat: x[\"bytes\"]})\n",
    "        \n",
    "        dfCs.append(dfC)\n",
    "        dfBs.append(dfB)\n",
    "    except:\n",
    "        print(\"WARNING\", i)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(11,6), sharey=True, sharex=True)\n",
    "\n",
    "graphs = [\n",
    "    (\"Mean Percentage of Classes Left\", dfCs, lambda x: x.mean()), \n",
    "    (\"Mean Percentage of Bytes Left\", dfBs, lambda x: x.mean()),\n",
    "    (\"Median Percentage of Classes Left\", dfCs, lambda x: x.median()), \n",
    "    (\"Median Percentage of Bytes Left\", dfBs, lambda x: x.median()),  \n",
    "]\n",
    "\n",
    "for ax, (title, dfc, fn) in zip(axes.flatten(), graphs):\n",
    "    m = fn(pd.concat(dfc, keys=range(0, len(dfc))).groupby(\"time\"))\n",
    "    for s in strategies:\n",
    "        ax.plot(m.index * 60, m[s], label=s, color=colors[s])\n",
    "        \n",
    "        v = ax.hlines(min(m[s]),0, 3600)\n",
    "        v.set_color(\"lightgray\")\n",
    "        v.set_linestyle(\":\")\n",
    "        \n",
    "    ax.set_ylim(0,1)\n",
    "    ax.set_xlim(0,3600)\n",
    "    ax.set_xticks(np.linspace(0,3600, 7))\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, pos: f'{x*100:0.0f}%'))\n",
    "    ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, pos: f'{x:0.0f} s'))\n",
    "    \n",
    "    pretty(ax)\n",
    "\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"new-approach.pdf\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(11,6), sharey=True, sharex=True)\n",
    "\n",
    "graphs = [\n",
    "    (\"Mean Percentage of Classes Left\", dfCs, lambda x: x.mean()), \n",
    "    (\"Mean Percentage of Bytes Left\", dfBs, lambda x: x.mean()),\n",
    "    (\"Median Percentage of Classes Left\", dfCs, lambda x: x.median()), \n",
    "    (\"Median Percentage of Bytes Left\", dfBs, lambda x: x.median()),  \n",
    "]\n",
    "\n",
    "for ax, (title, dfc, fn) in zip(axes.flatten(), graphs):\n",
    "    m = fn(pd.concat(dfc, keys=range(0, len(dfc))).groupby(\"time\"))\n",
    "    for s in strategies:\n",
    "        ax.plot(m.index * 60, m[s], label=s, color=colors[s])\n",
    "        \n",
    "        v = ax.hlines(min(m[s]),0, 3600)\n",
    "        v.set_color(\"lightgray\")\n",
    "        v.set_linestyle(\":\")\n",
    "        \n",
    "    ax.set_ylim(0,1)\n",
    "    ax.set_xlim(0,3600)\n",
    "    ax.set_xticks(np.linspace(0,3600, 7))\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, pos: f'{x*100:0.0f}%'))\n",
    "    ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, pos: f'{x:0.0f} s'))\n",
    "    \n",
    "    pretty(ax)\n",
    "\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"new-approach.pdf\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra evaluation\n",
    "\n",
    "Here i have left some space for some extra interesting questions: \n",
    "\n",
    "The first question is how the size of the input in bytes affect the time to setup and run the predicate. In this case it is fernflower.\n",
    "\n",
    "The interesting thing here is that the execution time of the predicate is dependent on the size of the input, and by testing small inputs it can be up to 10 times faster than testing the large inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    metrics = pd.read_csv(\"result/full/url0e7ea11f42_rbouckaert_DensiTree/fernflower/logic/workfolder/metrics.csv\")\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(7,7), sharex=True)\n",
    "\n",
    "\n",
    "    for key, ax in zip([\"setup time\", \"run time\"], axes):\n",
    "        ax.scatter(metrics.bytes, metrics[key])\n",
    "        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, pos: f'{x:0.1f} s'))\n",
    "        ax.set_xlim(0, metrics.bytes.max() * 1.1)\n",
    "        ax.set_ylim(0, metrics[key].max() * 1.1)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.set_title(key)\n",
    "        pretty(ax)\n",
    "        \n",
    "    ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, pos: f'{x/1000:0.0f} Kb'))\n",
    "    \n",
    "    fig.tight_layout()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part\n",
    "\n",
    "Here we analyse given only 1 bug being preserved by Javac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_results = pd.read_csv(\"result/part/result.csv\").set_index([\"name\", \"predicate\",\"strategy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(14,2))\n",
    "\n",
    "timeouts = (part_results.status == \"timeout\").groupby(\"strategy\").mean()\n",
    "\n",
    "ax.set_xlim(0, 100)\n",
    "\n",
    "pretty(ax)\n",
    "x = ax.barh(\n",
    "        strategies, \n",
    "        [timeouts[s] * 100 for s in strategies], \n",
    "        color=[colors[s] for s in strategies]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative reduction\n",
    "\n",
    "In our first experiment we are going to look at comparative final size, and time. We use the geometric mean, so that we can compare the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMEOUT = 3600\n",
    "part = part_results.copy()\n",
    "part.time.loc[part.status == \"timeout\"] = TIMEOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyvalues = part.filter([\"bytes\", \"classes\", \"time\"], axis=1)\\\n",
    "    .unstack(\"strategy\")\\\n",
    "    .agg(stats.gmean)\\\n",
    "    .unstack()\n",
    "\n",
    "v = part.filter([\"initial-bytes\", \"initial-classes\"], axis=1).unstack(\"strategy\")\\\n",
    "    .agg(stats.gmean)\\\n",
    "    .unstack()[\"classes\"]\\\n",
    "    .rename(lambda a: a.lstrip(\"initial-\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geometric Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyvalues.round(1)[list(reversed(strategies))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare them on how much reduction each of them have made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(keyvalues.loc[[\"bytes\",\"classes\"]].div(v, axis='rows') * 100).round(1)[list(reversed(strategies))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphical Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = draw_diagram(part)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
