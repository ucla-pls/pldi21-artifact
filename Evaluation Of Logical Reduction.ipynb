{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the notebook, which contains the results of running our evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from pathlib import Path\n",
    "\n",
    "def investigate(name, predicate, strategy):\n",
    "    folder = Path(\"result\") / name / predicate / strategy / \"workfolder\"\n",
    "    final = folder / \"final/sandbox\" / predicate \n",
    "    print((final / \"compiler.out.txt\").read_text())\n",
    "    for f in final.glob(\"src/**/*.java\"):\n",
    "        print(f)\n",
    "        print(f.read_text())\n",
    "        \n",
    "    print(\"=======\")\n",
    "    \n",
    "def check_verify(name, predicate, strategy):\n",
    "    folder = Path(\"result\") / name / predicate / strategy / \"workfolder\"\n",
    "    v = results.loc[(name, predicate, strategy)].verify\n",
    "    if v != \"success\":\n",
    "        a = set((folder / \"reduction\" / v / \"stdout\").read_text().splitlines())\n",
    "        b = set((folder / \"initial\" / \"stdout\").read_text().splitlines())\n",
    "        \n",
    "        print (a - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full\n",
    "\n",
    "This section covers the evaluation where we preserve the full bug. We start by loading the the data and indexing by `name`, `predicate`, and `strategy`. The data have been computed and put in `results/result.csv` by our evalutation framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(\"result/result.csv\").set_index([\"name\", \"predicate\",\"strategy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A single line of our data looks like this, we store the follwing data: \n",
    "\n",
    "*  `bugs` which contain the number of lines in the cleaned up bug-report\n",
    "\n",
    "*  `initial-scc` and `scc` contain the number of strongly connected components before and after reduction,\n",
    "\n",
    "*  `initial-classes` and `classes` contain the number of classes before and after reduction,\n",
    "\n",
    "*  `initial-bytes` and `bytes` contain the number of bytes before and after reduction,\n",
    "\n",
    "*  `iters` which contain the number of invocations of the predicate, \n",
    "\n",
    "*  `searches` the number of binary searches made by algorithm\n",
    "\n",
    "*  `time` which records the time to reach the final successfull solution,\n",
    "\n",
    "*  `status` which records whether the reduction completed correctly,\n",
    "\n",
    "*  `verify` which records information about if bug is preserved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[\"url0067cdd33d_goldolphin_Mi\", \"cfr\", \"classes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Sanity Checks\n",
    "\n",
    "Before we go on to evaluate the code we check that the system is working correctly. First we check that the status is \"success\". We find the following distribution of statuses:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15,5))\n",
    "    \n",
    "x = results.status.value_counts().plot.pie(ax=axes[0])\n",
    "x = results.verify.value_counts().plot.pie(ax=axes[1])\n",
    "x = results.flaky.value_counts().plot.pie(ax=axes[2])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a list of all the experiments that failed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in results[results.status != \"success\"].index:\n",
    "    print('/'.join(i), results.loc[i].status, results.bugs[i], results.searches[i], results.iters[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to make sure that we do not use more searches than bugs, as this probably means that we have an incomplete description of java."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in results[results.searches > results.bugs].index:\n",
    "    if i[2] != \"logic+extends+over\": continue \n",
    "    print('/'.join(i), int(results.bugs[i]), int(results.searches[i]), results.classes[i], results.verify[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in results[results.verify != \"success\"].index:\n",
    "    if i[2] != \"logic+extends+over\": continue \n",
    "    print('/'.join(i), int(results.bugs[i]), results.verify[i], int(results.searches[i]), results.classes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on we only work with data where all evaluation techniques succeed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success = results.status.apply(lambda x: x == \"success\").groupby([\"name\", \"predicate\"]).all()\n",
    "sucessfull = results[success[results.index]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative reduction\n",
    "\n",
    "In our first experiment we are going to look at comparative final size, and time. We use the geometric mean, so that we can compare the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyvalues = sucessfull.filter([\"bytes\", \"classes\", \"time\"], axis=1)\\\n",
    "    .unstack(\"strategy\")\\\n",
    "    .agg(stats.gmean)\\\n",
    "    .unstack()\n",
    "\n",
    "print(\"Geomertric averages:\")\n",
    "print(keyvalues.round(1))\n",
    "\n",
    "print(\"\\nRelative to classes:\")\n",
    "print(keyvalues.div(keyvalues['classes'], axis='rows').round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that on average the over apporixmation and under-aproximation performs 6 times better on number of bytes and 2 times better on number of classes, it does however take 3-4 times longer.\n",
    "\n",
    "We can get more detailed information by inspecting the graphs for runtimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(9,7), sharey=True)\n",
    "\n",
    "for lb, ax in zip([\"time\", \"iters\", \"bytes\", \"classes\"], axes.flatten()):\n",
    "    maxx = 0\n",
    "    x = results[success[results.index]][lb].unstack(\"strategy\")\n",
    "    for key in [\"logic+over\", \"logic+extends+over\", \"classes\"]:\n",
    "        ax.plot(sorted(x[key]), [i + 1 for i,_ in enumerate(x[key])], label=key)\n",
    "        maxx = max(maxx, max(x[key]))\n",
    "        \n",
    "    ylim = 1, len(x[key])\n",
    "    ax.set_yticks(np.linspace(*ylim, 6))\n",
    "    ax.set_ylim(*ylim)\n",
    "    \n",
    "    if lb == \"time\":\n",
    "        xlim = 0, 3200\n",
    "    else:\n",
    "        xlim = 0, maxx\n",
    "    ax.set_xticks(np.linspace(*xlim, 5))\n",
    "    ax.set_xlim(*xlim)\n",
    "    \n",
    "    if lb == \"bytes\":\n",
    "        ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, pos: f'{x/1000:0.0f} Kb'))\n",
    "    elif lb == \"time\" :\n",
    "        ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, pos: f'{x:0.0f} s'))\n",
    "    else:\n",
    "        ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, pos: f'{x:0.0f}'))\n",
    "    \n",
    "    ax.set_xlabel(lb)    \n",
    "    \n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    \n",
    "    for spine in [ax.spines['left'], ax.spines['bottom']]:\n",
    "        spine.set_position((\"outward\", 5))\n",
    "        spine.set_color(\"gray\")\n",
    "        \n",
    "    for axis in [ax.yaxis, ax.xaxis]:\n",
    "        for x in axis.get_major_ticks():\n",
    "            x.label1.set_color(\"gray\")\n",
    "            x.tick1line.set_color(\"gray\")\n",
    "\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphs are formatted like the previous article: In the top row we have number programs that complete before a certain time and iterations. In the bottom row we have the number of programs that have been reduced to a size below a certian number of bytes or classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra evaluation\n",
    "\n",
    "Here i have left some space for some extra interesting questions: \n",
    "\n",
    "The first question is how the size of the input in bytes affect the time to setup and run the predicate. In this case it is fernflower.\n",
    "\n",
    "The interesting thing here is that the execution time of the predicate is dependent on the size of the input, and by testing small inputs it can be up to 10 times faster than testing the large inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.read_csv(\"result/url0e7ea11f42_rbouckaert_DensiTree/fernflower/logic+over/workfolder/metrics.csv\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(7,7), sharex=True)\n",
    "\n",
    "\n",
    "for key, ax in zip([\"setup time\", \"run time\"], axes):\n",
    "    ax.scatter(metrics.bytes, metrics[key])\n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, pos: f'{x:0.1f} s'))\n",
    "    ax.set_xlim(0, metrics.bytes.max() * 1.1)\n",
    "    ax.set_ylim(0, metrics[key].max() * 1.1)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.set_title(key)\n",
    "    for spine in [ax.spines['left'], ax.spines['bottom']]:\n",
    "        spine.set_position((\"outward\", 5))\n",
    "        spine.set_color(\"gray\")\n",
    "        \n",
    "    for axis in [ax.yaxis, ax.xaxis]:\n",
    "        for x in axis.get_major_ticks():\n",
    "            x.label1.set_color(\"gray\")\n",
    "            x.tick1line.set_color(\"gray\")\n",
    "    \n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, pos: f'{x/1000:0.0f} Kb'))\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix. Here Be Dragons!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14,7), sharey=True)\n",
    "\n",
    "metrics = [ \"bytes\", \"classes\" ] \n",
    "\n",
    "labels = [f\"{a}:{b}\" for a, b in sucessfull.unstack(\"strategy\").index]\n",
    "x = np.arange(len(labels))\n",
    "total_width = 0.75\n",
    "  \n",
    "for metric, ax in zip(metrics, list(axes)): \n",
    "    m = sucessfull[metric].unstack(\"strategy\")\n",
    "    rest = 1 / m.drop([\"classes\"], axis=1).div(m.classes, axis=0, level=0)\n",
    "\n",
    "    width = total_width / len(rest.columns)\n",
    "   \n",
    "    minx, maxx = 1, 0\n",
    "    for n, i in enumerate(rest.columns):\n",
    "        offset = x - (total_width/2 - width*n - width/2)\n",
    "        ax.barh(offset,\n",
    "                [ 1 - x if x < 1 else x - 1 for x in rest[i] ], \n",
    "                height=width * 0.75, \n",
    "                left=[min(x, 1) for x in rest[i]], \n",
    "                label=i)\n",
    "        maxx = max(maxx, rest[i].max())\n",
    "\n",
    "        \n",
    "    ax.set_ylim(-total_width, len(labels) -1 + total_width)\n",
    "    ax.set_xlabel(metric)\n",
    "    ax.set_xlim(minx/2,maxx*2)\n",
    "    ax.set_xscale(\"log\")\n",
    "    #ax.set_xticks([1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1,2, 4, 8])\n",
    "    ax.xaxis.set_major_locator(plt.LogLocator(10))\n",
    "    ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, y: f\"{1/x}x\")) #plt.LogFormatterMathtext(2))\n",
    "    #ax.set_xticklabels([1/8, 1/4, 1/2,1,2, 4, 8])\n",
    "   \n",
    "    ax.axvline(1, ls='-', color='lightgray', lw=1)\n",
    "\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    ax.spines[\"left\"].set_position((\"outward\", 5))\n",
    "    ax.spines[\"bottom\"].set_position((\"outward\", 5))\n",
    "\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(labels)\n",
    "    \n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
